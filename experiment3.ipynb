{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb82dddb",
   "metadata": {},
   "source": [
    "# Experiment3\n",
    "`VAE+GAN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb801f",
   "metadata": {},
   "source": [
    "# 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783801db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from analysis_tools.common import *\n",
    "from util import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import sklearn\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "sklearn.random.seed(RANDOM_STATE)\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22739b65",
   "metadata": {},
   "source": [
    "# 2. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9365d",
   "metadata": {},
   "source": [
    "## 2.1 screw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc7b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample = 'screw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137dc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.7s\n"
     ]
    }
   ],
   "source": [
    "@delayed\n",
    "def load_img(path, size):\n",
    "    return cv2.resize(cv2.imread(path), (size, size))\n",
    "\n",
    "SIZE        = 128\n",
    "INPUT_SHAPE = (SIZE, SIZE, 3)\n",
    "train_full_data_meta = pd.read_csv(join(PATH.input, 'train_df.csv'), index_col=0)\n",
    "train_full_data_meta = train_full_data_meta.query(f\"`class` == '{class_sample}'\")\n",
    "paths                = train_full_data_meta['file_name']\n",
    "with ProgressBar():\n",
    "    X_train_full = np.array(compute(*[load_img(join(PATH.train, path), SIZE) for path in paths]), dtype=np.float32)\n",
    "    y_train_full = train_full_data_meta['state'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34392eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (180, 128, 128, 3)\n",
      "X_val: (60, 128, 128, 3)\n",
      "X_test: (141, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_normal   = X_train_full[y_train_full == 'good']\n",
    "X_abnormal = X_train_full[y_train_full != 'good']\n",
    "X_train, X_test = train_test_split(X_normal)\n",
    "X_train, X_val  = train_test_split(X_train)\n",
    "X_test = np.concatenate([X_test, X_abnormal])\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_val   = X_val / 255\n",
    "X_test  = X_test / 255\n",
    "\n",
    "print(\"X_train:\", X_train.shape)  # normal\n",
    "print(\"X_val:\", X_val.shape)      # noraml + abnormal\n",
    "print(\"X_test:\", X_test.shape)    # noraml + abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5f3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# aug_model = keras.models.Sequential([\n",
    "#     keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
    "#     keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "# ])\n",
    "\n",
    "# def preprocess(ds, training, batch_size, augment=True):\n",
    "#     ds = ds.cache().batch(batch_size)\n",
    "#     if training:\n",
    "#         ds = ds.shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)\n",
    "#         if augment:\n",
    "#             ds = ds.map(lambda x, y: (aug_model(x), aug_model(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     return ds\n",
    "\n",
    "def trans1(img):\n",
    "    return tfa.image.rotate(tf.image.flip_left_right(tf.image.flip_up_down(img)), -0.2, fill_mode='reflect', interpolation='bilinear')\n",
    "def trans2(img):\n",
    "    return tfa.image.rotate(img, -0.2, fill_mode='reflect', interpolation='bilinear')\n",
    "def trans3(img):\n",
    "    return tfa.image.rotate(img, 0.2, fill_mode='reflect', interpolation='bilinear')\n",
    "\n",
    "def preprocess(ds):\n",
    "    ds = ds.shuffle(buffer_size=1000).batch(BATCH_SIZE)\n",
    "    ds1, ds2, ds3 = ds.map(trans1), ds.map(trans2), ds.map(trans3)\n",
    "    ds = ds.concatenate(ds1).concatenate(ds2).concatenate(ds3)\n",
    "    return ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "ds_train = preprocess(tf.data.Dataset.from_tensor_slices(X_train))\n",
    "ds_val   = preprocess(tf.data.Dataset.from_tensor_slices(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e97ea3",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e268d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from functools import partial\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5*z_log_var)*epsilon\n",
    "    \n",
    "conv  = partial(layers.Conv2D, kernel_size=3, strides=2, padding='same', kernel_initializer='lecun_normal', activation='selu')\n",
    "convt = partial(layers.Conv2DTranspose, kernel_size=3, strides=2, padding='same', kernel_initializer='lecun_normal', activation='selu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40576bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rounded_accuracy(y_true, y_pred):\n",
    "#     return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
    "\n",
    "# LATENT_DIM  = 32\n",
    "# SIZE        = 256\n",
    "# INPUT_SHAPE = (SIZE, SIZE, 3)\n",
    "\n",
    "# # Encoder\n",
    "# encoder_input = layers.Input(INPUT_SHAPE)\n",
    "# x = conv(32)(encoder_input)\n",
    "# for filters in (64, 128):\n",
    "#     x = conv(filters)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(256, activation='tanh')(x)\n",
    "# z_mean    = layers.Dense(LATENT_DIM, name='z_mean')(x)\n",
    "# z_log_var = layers.Dense(LATENT_DIM, name='z_log_var')(x)\n",
    "# z = Sampling()([z_mean, z_log_var])\n",
    "# encoder = keras.Model(inputs=encoder_input, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "# # Decoder\n",
    "# latent_input = layers.Input(LATENT_DIM)\n",
    "# x = layers.Dense(32*32*64, activation='selu', kernel_initializer='lecun_normal')(latent_input)\n",
    "# x = layers.Reshape((32, 32, 64))(x)\n",
    "# for filters in (128, 64):\n",
    "#     x = convt(filters)(x)\n",
    "# decoder_output = convt(3, activation='sigmoid', kernel_initializer='glorot_normal')(x)    \n",
    "# decoder = keras.Model(inputs=latent_input, outputs=decoder_output, name='decoder')\n",
    "\n",
    "# encoder.summary()\n",
    "# decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae9a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator_input = layers.Input(INPUT_SHAPE)\n",
    "# x = conv(128)(discriminator_input)\n",
    "# x = conv(64)(x)\n",
    "# x = conv(32)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "# discriminator = keras.Model(inputs=discriminator_input, outputs=discriminator_output, name='discriminator')\n",
    "# discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cdf9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 32)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   18496       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 128)  73856       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 256)    295168      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 16384)        0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 160)          2621600     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           5152        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 32)           5152        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 32)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,021,216\n",
      "Trainable params: 3,020,768\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              135168    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 256)       65792     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 128)       131200    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 64)        32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 128, 128, 32)      8224      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 3)       867       \n",
      "=================================================================\n",
      "Total params: 1,159,715\n",
      "Trainable params: 1,158,819\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 128)       24704     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 32)          32800     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 328,929\n",
      "Trainable params: 328,545\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
    "\n",
    "LATENT_DIM  = 32\n",
    "\n",
    "encoder_inputs = keras.Input(INPUT_SHAPE)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(256, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(160, activation=\"tanh\")(x)\n",
    "z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(LATENT_DIM)\n",
    "x = layers.Dense(8* 8 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((8, 8, 64))(x)\n",
    "x = layers.Conv2DTranspose(256, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(256, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(128, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(128, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(64, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(32, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator_inputs = keras.Input(INPUT_SHAPE)\n",
    "x = layers.Conv2D(128, 8, activation=\"relu\", strides=2, padding=\"same\")(discriminator_inputs)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, 5, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(32, 4, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "discriminator_outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "discriminator = keras.Model(discriminator_inputs, discriminator_outputs, name=\"discriminator\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72f2b20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def rounded_accuracy(y_true, y_pred):\n",
    "#     return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
    "\n",
    "# LATENT_DIM  = 32\n",
    "\n",
    "# encoder_inputs = keras.Input(INPUT_SHAPE)\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\", use_bias=False)(encoder_inputs)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(256, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(160, activation=\"tanh\")(x)\n",
    "# z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
    "# z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
    "# z = Sampling()([z_mean, z_log_var])\n",
    "# encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "# latent_inputs = keras.Input(LATENT_DIM)\n",
    "# x = layers.Dense(16 * 16 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((16, 16, 64))(x)\n",
    "# x = layers.Conv2DTranspose(256, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2D(256, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2DTranspose(128, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2D(128, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2DTranspose(64, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2DTranspose(32, 2, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "# decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "# decoder.summary()\n",
    "\n",
    "# discriminator_inputs = keras.Input(INPUT_SHAPE)\n",
    "# x = layers.Conv2D(128, 8, activation=\"relu\", strides=2, padding=\"same\")(discriminator_inputs)\n",
    "# x = layers.MaxPool2D()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(64, 5, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.MaxPool2D()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(32, 4, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(128, activation=\"relu\")(x)\n",
    "# discriminator_outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "# discriminator = keras.Model(discriminator_inputs, discriminator_outputs, name=\"discriminator\")\n",
    "# discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449ef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_loss(z):\n",
    "    corr_matrix = tfp.stats.correlation(z)\n",
    "    n = corr_matrix.shape[0]\n",
    "    loss = tf.reduce_sum(corr_matrix**2)\n",
    "    for i, j in product(range(n), range(n)):\n",
    "        loss -= corr_matrix[i, i]**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da83f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z, reconstruction\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction       = self.decoder(z)\n",
    "            reconstruction_loss  = tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)))\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - z_mean**2 - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {'loss': self.total_loss_tracker.result(), 'reconstruction_loss': self.reconstruction_loss_tracker.result(), 'kl_loss': self.kl_loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1acb027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class VAE_GAN(keras.Model):\n",
    "    def __init__(self, vae, discriminator, opti1=keras.optimizers.Adam(), opti2=keras.optimizers.Adam(), opti3=keras.optimizers.Adam(), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vae           = vae\n",
    "        self.discriminator = discriminator\n",
    "        self.encoder       = vae.encoder\n",
    "        self.decoder       = vae.decoder\n",
    "\n",
    "        self.vae_loss_tracker            = keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker             = keras.metrics.Mean(name='kl_loss')\n",
    "        self.correlation_loss_tracker    = keras.metrics.Mean(name=\"cr_loss\")\n",
    "        self.disc_loss_tracker           = keras.metrics.Mean(name='disc_loss')\n",
    "        self.gen_loss_tracker            = keras.metrics.Mean(name='gen_loss')\n",
    "        self.disc_loss                   = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        self.vae_optimizer  = opti1\n",
    "        self.gen_optimizer  = opti2\n",
    "        self.disc_optimizer = opti3\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z, reconstruction\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.vae_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker, self.correlation_loss_tracker, self.disc_loss_tracker, self.gen_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        batch_size = K.shape(data)[0]\n",
    "\n",
    "        with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "            # VAE\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)))\n",
    "            kl_loss             = -0.5*(1 + z_log_var - z_mean**2 - tf.exp(z_log_var))\n",
    "            kl_loss             = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            correlation_loss    = corr_loss(z)\n",
    "\n",
    "            # GAN\n",
    "            recon_vect = z\n",
    "            construction = self.decoder(recon_vect)\n",
    "            combined_images = tf.concat([data, construction], axis=0)\n",
    "            data_l, recon_l = tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))  # 0: real, 1: fake\n",
    "            combined_l = tf.concat([data_l, recon_l], axis=0)\n",
    "            tot_predictions = self.discriminator(combined_images)\n",
    "            r_prediction = self.discriminator(construction)\n",
    "\n",
    "            discr_loss = self.disc_loss(combined_l, tot_predictions)\n",
    "            gen_loss   = tf.maximum(self.disc_loss(data_l, r_prediction) - discr_loss, 1e-4)\n",
    "            vae_loss   = reconstruction_loss + kl_loss + gen_loss\n",
    "\n",
    "            grad_discr = disc_tape.gradient(discr_loss, self.discriminator.trainable_weights)\n",
    "            grad_vae = enc_tape.gradient(vae_loss, self.vae.trainable_weights)\n",
    "\n",
    "            self.disc_optimizer.apply_gradients(zip(grad_discr, self.discriminator.trainable_weights))\n",
    "            self.vae_optimizer.apply_gradients(zip(grad_vae, self.vae.trainable_weights))\n",
    "\n",
    "            self.vae_loss_tracker.update_state(vae_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            self.correlation_loss_tracker.update_state(correlation_loss)\n",
    "            self.disc_loss_tracker.update_state(discr_loss)\n",
    "            self.gen_loss_tracker.update_state(gen_loss)\n",
    "\n",
    "        return {'vae_loss': self.vae_loss_tracker.result(), 'disc_loss': self.disc_loss_tracker.result(), 'gen_loss': self.gen_loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8ab553",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae   = VAE(encoder, decoder)\n",
    "model = VAE_GAN(vae, discriminator)\n",
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ffac2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - 17s 110ms/step - vae_loss: 10804.4875 - disc_loss: 0.6254 - gen_loss: 0.1687\n",
      "Epoch 2/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9631.4865 - disc_loss: 0.6933 - gen_loss: 0.2284\n",
      "Epoch 3/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9552.8091 - disc_loss: 0.4570 - gen_loss: 0.6275\n",
      "Epoch 4/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9525.5814 - disc_loss: 0.1851 - gen_loss: 2.3220\n",
      "Epoch 5/500\n",
      "24/24 [==============================] - 1s 53ms/step - vae_loss: 9509.1793 - disc_loss: 0.0619 - gen_loss: 2.9712\n",
      "Epoch 6/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9492.5765 - disc_loss: 0.1385 - gen_loss: 2.6332\n",
      "Epoch 7/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9483.2906 - disc_loss: 0.1754 - gen_loss: 2.4142\n",
      "Epoch 8/500\n",
      "24/24 [==============================] - 1s 53ms/step - vae_loss: 9399.4804 - disc_loss: 0.0832 - gen_loss: 3.0576\n",
      "Epoch 9/500\n",
      "24/24 [==============================] - 1s 54ms/step - vae_loss: 9312.8877 - disc_loss: 0.1079 - gen_loss: 3.6190\n",
      "Epoch 10/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9295.3300 - disc_loss: 0.0466 - gen_loss: 4.2803\n",
      "Epoch 11/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9248.7220 - disc_loss: 0.0897 - gen_loss: 4.5515\n",
      "Epoch 12/500\n",
      "24/24 [==============================] - 1s 52ms/step - vae_loss: 9226.9638 - disc_loss: 0.0459 - gen_loss: 4.3955\n",
      "Epoch 13/500\n",
      " 5/24 [=====>........................] - ETA: 1s - vae_loss: 9122.6275 - disc_loss: 0.0156 - gen_loss: 4.5511"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fbe83dfc626c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , callbacks=[EarlyStopping(patience=20, monitor='val_total_loss')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "H = model.fit(ds_train, epochs=500, verbose=1)  # , callbacks=[EarlyStopping(patience=20, monitor='val_total_loss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "_ ,digit_size = INPUT_SHAPE[:-1]\n",
    "scale = 1\n",
    "def plot_latent_space(vae, n=8, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    figure = np.zeros((digit_size * n, digit_size * n,3))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[2*random.random()-1 for i in range(LATENT_DIM)]])\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size,3)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_size, _ = INPUT_SHAPE[:-1]\n",
    "n = 4\n",
    "figure = np.zeros((digit_size*3, digit_size * n,3))\n",
    "img = list(ds_train)[0]\n",
    "\n",
    "for i in range(n):\n",
    "    _,b_img = model(img)\n",
    "    a = list(b_img)[i]\n",
    "    figure[\n",
    "                 0*digit_size :  digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = a\n",
    "    figure[\n",
    "                 1*digit_size :  2*digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = list(img)[i]\n",
    "    \n",
    "    figure[\n",
    "                 2*digit_size :  3*digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = (a-list(img)[i])*10\n",
    "\n",
    "\n",
    "figsize = 5   \n",
    "plt.figure(figsize=(figsize*n, figsize*3))\n",
    "plt.imshow(figure)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9704b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "figure = np.zeros((digit_size*3, digit_size * n,3))\n",
    "img = X_test[:10]\n",
    "for i in range(n):\n",
    "    _,b_img = model(img)\n",
    "    a = list(b_img)[i]\n",
    "    figure[\n",
    "                 0*digit_size :  digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = a\n",
    "    figure[\n",
    "                 1*digit_size :  2*digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = list(img)[i]\n",
    "    \n",
    "    figure[\n",
    "                 2*digit_size :  3*digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = (a-list(img)[i])*10\n",
    "\n",
    "figsize = 5  \n",
    "plt.figure(figsize=(figsize*n, figsize*3))\n",
    "plt.axis('off')\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "figure = np.zeros((digit_size*3, digit_size * n,3))\n",
    "img = X_test[-11:]\n",
    "for i in range(n):\n",
    "    _,b_img = model(img)\n",
    "    a = list(b_img)[i]\n",
    "    figure[\n",
    "                 0*digit_size :  digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = a\n",
    "    figure[\n",
    "                 1*digit_size :  2*digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = list(img)[i]\n",
    "    \n",
    "    figure[\n",
    "                 2*digit_size :  3*digit_size,\n",
    "                i * digit_size :  (i+1)* digit_size,\n",
    "            ] = (a-list(img)[i])*10\n",
    "\n",
    "figsize = 5  \n",
    "plt.figure(figsize=(figsize*n, figsize*3))\n",
    "plt.axis('off')\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
